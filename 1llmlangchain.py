# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TVogurQZM48qM2U7F07ZPAPNtdd3W5Nm
"""

# âœ… Install required libraries
!pip install openai langchain-openai langchain faiss-cpu gradio

# âœ… Import necessary libraries
import os
import openai
import gradio as gr
from langchain.llms import OpenAI
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings  # Corrected import for embeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQA

# ğŸš¨ Provide Your OpenAI API Key Here ğŸš¨
openai_api_key = "your_openai_api_key_here"  # ğŸ”¹ Replace this with your OpenAI API key
os.environ["OPENAI_API_KEY"] = openai_api_key  # ğŸ”¹ Store API key securely in environment variable

# âœ… Ensure API key exists before proceeding
if not openai_api_key:
    raise ValueError("âŒ API key missing! Please set OPENAI_API_KEY securely.")

# âœ… Load a sample AI-related document
document_text = """
AI is transforming industries like healthcare, finance, and education.
Machine learning algorithms analyze datasets to detect patterns and make predictions.
Natural language processing enables AI to generate human-like text.
Robotics and AI together create automation solutions that improve efficiency.
"""

# âœ… Split text into smaller chunks for embeddings
text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)
documents = text_splitter.split_text(document_text)

# âœ… Create embeddings and FAISS index
embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)  # ğŸ”¹ API key is required here
vector_store = FAISS.from_texts(documents, embeddings)

# âœ… Set up retrieval-based question answering (QA)
retriever = vector_store.as_retriever()
qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(model_name="gpt-4", openai_api_key=openai_api_key), retriever=retriever)

# âœ… Define a function to answer questions using retrieval + GPT-4
def ask_question(query):
    response = qa_chain.run(query)
    return response

# âœ… Create Gradio interface
demo = gr.Interface(
    fn=ask_question,
    inputs=gr.Textbox(lines=2, placeholder="Ask about AI..."),
    outputs="text",
    title="LLM + Vector Search: AI Knowledge Finder",
    description="Retrieves relevant AI knowledge and provides responses using GPT-4."
)

# âœ… Launch the Gradio app
demo.launch()